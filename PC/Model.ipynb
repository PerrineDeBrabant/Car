{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#Declare different variables\n",
    "#Make variable shape with the width height ansd channels of the image \n",
    "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 240, 320, 3\n",
    "SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n",
    "#Give up your path where all your image files that you want to use for training are located\n",
    "RAW_PATH = 'D:/S1 2018-2019/ProjectIV/train/'\n",
    "PATH = RAW_PATH\n",
    "#In the script for your images you see that they always begin with agg_filez so select all the files that start with this name and call them your train files\n",
    "TRAIN_DATA_FILES = PATH+'agg_filez*.pkl'\n",
    "#Make a directory called keras_models inside the map where your images are, here you will give up the path to that file.\n",
    "MODEL_PATH = RAW_PATH + 'keras_models/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(TRAIN_DATA_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use the images we need to load them\n",
    "def load_train_data(_train_files):\n",
    "\t#load the data and check the total size of it\n",
    "\ttraining_data = glob.glob(_train_files)\n",
    "\tt, l = load_file(training_data[0])\n",
    "\ttrain_size = len(t)*len(training_data)\n",
    "\tprint('\\nTotal size training data: '+str(train_size))\n",
    "\n",
    "\t# Start array make one for training data and their labels\n",
    "\ttotal_train = np.zeros([train_size,240,320,3])\n",
    "\ttotal_label = np.zeros([train_size,3])\n",
    "\ti = 0\n",
    "\n",
    "\tprint('Array ready, start loading data')\n",
    "\tfor pkl in training_data:\t\n",
    "\t\tprint('\\n\tloading: '+str(pkl))\n",
    "\t\tt = []\n",
    "\t\tl = []\n",
    "\t\tt, l = load_file(pkl)\n",
    "\t\tfor index in range(len(t)):\n",
    "\t\t\ttotal_train[i] = t[index]\n",
    "\t\t\ttotal_label[i] = l[index]\n",
    "\t\t\ti += 1\n",
    "\n",
    "\treturn total_train, total_label\n",
    "\n",
    "#load your file\n",
    "def load_file(_file):\n",
    "    with open(_file,'rb') as f:  \n",
    "        t, l = pickle.load(f)\n",
    "    return t, l\n",
    "\n",
    "\n",
    "\n",
    "def print_step(_msg,_time):\n",
    "\tprint(_msg)\n",
    "\tprint_time(_time,time.time())\n",
    "\n",
    "\n",
    "def print_time(_start,_end):\n",
    "\ttime = round((_end - _start),2)\n",
    "\tprint((\">>> Time take: %s \\n\")%(time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading data...\n",
      "\n",
      "Total size training data: 798\n",
      "Array ready, start loading data\n",
      "\n",
      "\tloading: D:/S1 2018-2019/ProjectIV/train\\agg_filez.pkl\n",
      "\n",
      "\tloading: D:/S1 2018-2019/ProjectIV/train\\agg_filez_2.pkl\n",
      "\n",
      "\tloading: D:/S1 2018-2019/ProjectIV/train\\agg_filez_color.pkl\n",
      "\n",
      "\tloading: D:/S1 2018-2019/ProjectIV/train\\agg_filez_color2.pkl\n",
      "\n",
      "\tloading: D:/S1 2018-2019/ProjectIV/train\\agg_filez_flipped.pkl\n",
      "\n",
      "\tloading: D:/S1 2018-2019/ProjectIV/train\\agg_filez_flipped_color.pkl\n",
      "\n",
      "\tloading: D:/S1 2018-2019/ProjectIV/train\\agg_filez_flipped_color2.pkl\n",
      "Data loaded! X shape: (798, 240, 320, 3) and y shape: (798, 3)\n",
      ">>> Time take: 4.83 \n",
      "\n",
      ">> Separating test and train...\n",
      "Train split successful! Train shape: (638, 240, 320, 3) and Train_label shape: (638, 3) \n",
      "\n",
      ">>> Time take: 0.75 \n",
      "\n",
      ">> Defining model...\n"
     ]
    }
   ],
   "source": [
    "print(\">> Loading data...\")\n",
    "e0 = time.time()\n",
    "#load your training data\n",
    "X, y = load_train_data(TRAIN_DATA_FILES)\n",
    "print_step((('Data loaded! X shape: %s and y shape: %s')%(X.shape,y.shape)),e0)\n",
    "\n",
    "# Separate into Train and test\n",
    "print(\">> Separating test and train...\")\n",
    "e1 = time.time()\n",
    "#plit your data in a train and test set do this for your data and your labels\n",
    "train, test, train_labels, test_labels = train_test_split(X,y, test_size = 0.2)\n",
    "print_step((('Train split successful! Train shape: %s and Train_label shape: %s \\n')%(train.shape,train_labels.shape)),e1)\n",
    "\n",
    "print(\">> Defining model...\")\n",
    "e2 = time.time()\n",
    "\n",
    "#Beceaus you don't know which variables will define the best model you can try the different possibilities\n",
    "learning_rates = [0.001]\n",
    "#0.0001,0.00001\n",
    "batch_sizes = [32,64,128]\n",
    "optz_choices = ['Adam','SGD']\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 240, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 118, 158, 36)      2736      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 57, 77, 24)        21624     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 27, 37, 48)        28848     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 35, 64)        27712     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 23, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 23, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 48576)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               4857700   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 4,980,751\n",
      "Trainable params: 4,980,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model defined!\n",
      ">>> Time take: 20.18 \n",
      "\n",
      ">> Compiling model...\n",
      "Model compiled!\n",
      ">>> Time take: 0.05 \n",
      "\n",
      ">> Fiting model...\n",
      "Train on 638 samples, validate on 160 samples\n",
      "Epoch 1/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 27.4076 - acc: 0.3668 - val_loss: 5.3366 - val_acc: 0.5250\n",
      "Epoch 2/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 3.1324 - acc: 0.3292 - val_loss: 0.4072 - val_acc: 0.5500\n",
      "Epoch 3/25\n",
      "638/638 [==============================] - 28s 43ms/step - loss: 0.4034 - acc: 0.4890 - val_loss: 0.2207 - val_acc: 0.4562\n",
      "Epoch 4/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.2178 - acc: 0.5313 - val_loss: 0.1774 - val_acc: 0.6125\n",
      "Epoch 5/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.2184 - acc: 0.5580 - val_loss: 0.1711 - val_acc: 0.6250\n",
      "Epoch 6/25\n",
      "638/638 [==============================] - 27s 42ms/step - loss: 0.1725 - acc: 0.6630 - val_loss: 0.1540 - val_acc: 0.7000\n",
      "Epoch 7/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1655 - acc: 0.6755 - val_loss: 0.1426 - val_acc: 0.7125\n",
      "Epoch 8/25\n",
      "638/638 [==============================] - 28s 43ms/step - loss: 0.1539 - acc: 0.7006 - val_loss: 0.1412 - val_acc: 0.7312\n",
      "Epoch 9/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1498 - acc: 0.7069 - val_loss: 0.1398 - val_acc: 0.7562\n",
      "Epoch 10/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1528 - acc: 0.7210 - val_loss: 0.1273 - val_acc: 0.7750\n",
      "Epoch 11/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1540 - acc: 0.7147 - val_loss: 0.1334 - val_acc: 0.7688\n",
      "Epoch 12/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1444 - acc: 0.7288 - val_loss: 0.1475 - val_acc: 0.7438\n",
      "Epoch 13/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1333 - acc: 0.7524 - val_loss: 0.1277 - val_acc: 0.7812\n",
      "Epoch 14/25\n",
      "638/638 [==============================] - 28s 43ms/step - loss: 0.1349 - acc: 0.7429 - val_loss: 0.1194 - val_acc: 0.7937\n",
      "Epoch 15/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1256 - acc: 0.7508 - val_loss: 0.1129 - val_acc: 0.7937\n",
      "Epoch 16/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1222 - acc: 0.7649 - val_loss: 0.1210 - val_acc: 0.8313\n",
      "Epoch 17/25\n",
      "638/638 [==============================] - 31s 48ms/step - loss: 0.1256 - acc: 0.7649 - val_loss: 0.1066 - val_acc: 0.7875\n",
      "Epoch 18/25\n",
      "638/638 [==============================] - 29s 45ms/step - loss: 0.1215 - acc: 0.7806 - val_loss: 0.1057 - val_acc: 0.8125\n",
      "Epoch 19/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 0.1201 - acc: 0.7821 - val_loss: 0.1079 - val_acc: 0.7937\n",
      "Epoch 20/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 0.1481 - acc: 0.7382 - val_loss: 0.1030 - val_acc: 0.7812\n",
      "Epoch 21/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 0.1280 - acc: 0.7602 - val_loss: 0.1356 - val_acc: 0.7312\n",
      "Epoch 22/25\n",
      "638/638 [==============================] - 30s 47ms/step - loss: 0.1180 - acc: 0.7743 - val_loss: 0.1011 - val_acc: 0.8187\n",
      "Epoch 23/25\n",
      "638/638 [==============================] - 30s 48ms/step - loss: 0.1144 - acc: 0.7994 - val_loss: 0.1091 - val_acc: 0.8063\n",
      "Epoch 24/25\n",
      "638/638 [==============================] - 30s 47ms/step - loss: 0.1113 - acc: 0.7900 - val_loss: 0.1050 - val_acc: 0.8250\n",
      "Epoch 25/25\n",
      "638/638 [==============================] - 30s 46ms/step - loss: 0.1055 - acc: 0.8072 - val_loss: 0.0955 - val_acc: 0.8375\n",
      "Fit finished!\n",
      ">>> Time take: 707.89 \n",
      "\n",
      ">> Evaluating model...\n",
      "638/638 [==============================] - 12s 19ms/step\n",
      "\n",
      " \tEvaluate Train: acc: 83.54%\n",
      "160/160 [==============================] - 3s 19ms/step\n",
      "\n",
      " \tEvaluate Test: acc: 83.75%\n",
      "Evaluation finished!\n",
      ">>> Time take: 15.1 \n",
      "\n",
      ">> Saving model...\n",
      "Model saved!\n",
      ">>> Time take: 0.61 \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_2 (Lambda)            (None, 240, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 118, 158, 36)      2736      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 57, 77, 24)        21624     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 27, 37, 48)        28848     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 25, 35, 64)        27712     \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 23, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 23, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 48576)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               4857700   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 4,980,751\n",
      "Trainable params: 4,980,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model defined!\n",
      ">>> Time take: 744.0 \n",
      "\n",
      ">> Compiling model...\n",
      "Model compiled!\n",
      ">>> Time take: 0.05 \n",
      "\n",
      ">> Fiting model...\n",
      "Train on 638 samples, validate on 160 samples\n",
      "Epoch 1/25\n",
      "638/638 [==============================] - 28s 45ms/step - loss: 0.2675 - acc: 0.4608 - val_loss: 0.1894 - val_acc: 0.6250\n",
      "Epoch 2/25\n",
      "638/638 [==============================] - 28s 45ms/step - loss: 0.2216 - acc: 0.4875 - val_loss: 0.1728 - val_acc: 0.5750\n",
      "Epoch 3/25\n",
      "638/638 [==============================] - 28s 45ms/step - loss: 0.2020 - acc: 0.5549 - val_loss: 0.1650 - val_acc: 0.5750\n",
      "Epoch 4/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1888 - acc: 0.5721 - val_loss: 0.1585 - val_acc: 0.5938\n",
      "Epoch 5/25\n",
      "638/638 [==============================] - 29s 45ms/step - loss: 0.1802 - acc: 0.5878 - val_loss: 0.1547 - val_acc: 0.6937\n",
      "Epoch 6/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1725 - acc: 0.6176 - val_loss: 0.1520 - val_acc: 0.6250\n",
      "Epoch 7/25\n",
      "638/638 [==============================] - 29s 45ms/step - loss: 0.1655 - acc: 0.6505 - val_loss: 0.1467 - val_acc: 0.7188\n",
      "Epoch 8/25\n",
      "638/638 [==============================] - 28s 45ms/step - loss: 0.1617 - acc: 0.6630 - val_loss: 0.1440 - val_acc: 0.6937\n",
      "Epoch 9/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1612 - acc: 0.6991 - val_loss: 0.1418 - val_acc: 0.7188\n",
      "Epoch 10/25\n",
      "638/638 [==============================] - 28s 45ms/step - loss: 0.1559 - acc: 0.7085 - val_loss: 0.1393 - val_acc: 0.7250\n",
      "Epoch 11/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1553 - acc: 0.6959 - val_loss: 0.1377 - val_acc: 0.7438\n",
      "Epoch 12/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1505 - acc: 0.6991 - val_loss: 0.1353 - val_acc: 0.7750\n",
      "Epoch 13/25\n",
      "638/638 [==============================] - 27s 42ms/step - loss: 0.1494 - acc: 0.7194 - val_loss: 0.1347 - val_acc: 0.7500\n",
      "Epoch 14/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1495 - acc: 0.7038 - val_loss: 0.1325 - val_acc: 0.7750\n",
      "Epoch 15/25\n",
      "638/638 [==============================] - 28s 43ms/step - loss: 0.1434 - acc: 0.7524 - val_loss: 0.1296 - val_acc: 0.8000\n",
      "Epoch 16/25\n",
      "638/638 [==============================] - 30s 46ms/step - loss: 0.1412 - acc: 0.7226 - val_loss: 0.1278 - val_acc: 0.7875\n",
      "Epoch 17/25\n",
      "638/638 [==============================] - 30s 48ms/step - loss: 0.1438 - acc: 0.7335 - val_loss: 0.1262 - val_acc: 0.8187\n",
      "Epoch 18/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1400 - acc: 0.7335 - val_loss: 0.1241 - val_acc: 0.8063\n",
      "Epoch 19/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 0.1352 - acc: 0.7367 - val_loss: 0.1232 - val_acc: 0.8000\n",
      "Epoch 20/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1366 - acc: 0.7492 - val_loss: 0.1248 - val_acc: 0.7125\n",
      "Epoch 21/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 0.1313 - acc: 0.7555 - val_loss: 0.1210 - val_acc: 0.8063\n",
      "Epoch 22/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1328 - acc: 0.7649 - val_loss: 0.1182 - val_acc: 0.8313\n",
      "Epoch 23/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1289 - acc: 0.7821 - val_loss: 0.1177 - val_acc: 0.7937\n",
      "Epoch 24/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1311 - acc: 0.7539 - val_loss: 0.1163 - val_acc: 0.8063\n",
      "Epoch 25/25\n",
      "638/638 [==============================] - 28s 43ms/step - loss: 0.1269 - acc: 0.7586 - val_loss: 0.1140 - val_acc: 0.8125\n",
      "Fit finished!\n",
      ">>> Time take: 706.92 \n",
      "\n",
      ">> Evaluating model...\n",
      "638/638 [==============================] - 11s 18ms/step\n",
      "\n",
      " \tEvaluate Train: acc: 82.29%\n",
      "160/160 [==============================] - 3s 18ms/step\n",
      "\n",
      " \tEvaluate Test: acc: 81.25%\n",
      "Evaluation finished!\n",
      ">>> Time take: 14.14 \n",
      "\n",
      ">> Saving model...\n",
      "Model saved!\n",
      ">>> Time take: 0.47 \n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_3 (Lambda)            (None, 240, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 118, 158, 36)      2736      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 57, 77, 24)        21624     \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 27, 37, 48)        28848     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 25, 35, 64)        27712     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 23, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 23, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 48576)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               4857700   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 4,980,751\n",
      "Trainable params: 4,980,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model defined!\n",
      ">>> Time take: 1465.73 \n",
      "\n",
      ">> Compiling model...\n",
      "Model compiled!\n",
      ">>> Time take: 0.05 \n",
      "\n",
      ">> Fiting model...\n",
      "Train on 638 samples, validate on 160 samples\n",
      "Epoch 1/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 69.5749 - acc: 0.2837 - val_loss: 3.0553 - val_acc: 0.5500\n",
      "Epoch 2/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 1.6279 - acc: 0.4122 - val_loss: 0.6496 - val_acc: 0.5500\n",
      "Epoch 3/25\n",
      "638/638 [==============================] - 30s 46ms/step - loss: 0.6199 - acc: 0.4357 - val_loss: 0.4078 - val_acc: 0.2625\n",
      "Epoch 4/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.3520 - acc: 0.4248 - val_loss: 0.3004 - val_acc: 0.2625\n",
      "Epoch 5/25\n",
      "638/638 [==============================] - 29s 45ms/step - loss: 0.2256 - acc: 0.5094 - val_loss: 0.1678 - val_acc: 0.6750\n",
      "Epoch 6/25\n",
      "638/638 [==============================] - 29s 46ms/step - loss: 0.1764 - acc: 0.6034 - val_loss: 0.1525 - val_acc: 0.6188\n",
      "Epoch 7/25\n",
      "638/638 [==============================] - 30s 48ms/step - loss: 0.1637 - acc: 0.6567 - val_loss: 0.1496 - val_acc: 0.6438\n",
      "Epoch 8/25\n",
      "638/638 [==============================] - 31s 48ms/step - loss: 0.1734 - acc: 0.6223 - val_loss: 0.1396 - val_acc: 0.6500\n",
      "Epoch 9/25\n",
      "638/638 [==============================] - 31s 49ms/step - loss: 0.1477 - acc: 0.6959 - val_loss: 0.1415 - val_acc: 0.6562\n",
      "Epoch 10/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1473 - acc: 0.6944 - val_loss: 0.1411 - val_acc: 0.7125\n",
      "Epoch 11/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1471 - acc: 0.7132 - val_loss: 0.1471 - val_acc: 0.7188\n",
      "Epoch 12/25\n",
      "638/638 [==============================] - 28s 45ms/step - loss: 0.1440 - acc: 0.7241 - val_loss: 0.1252 - val_acc: 0.7688\n",
      "Epoch 13/25\n",
      "638/638 [==============================] - 27s 43ms/step - loss: 0.1496 - acc: 0.7210 - val_loss: 0.1296 - val_acc: 0.7375\n",
      "Epoch 14/25\n",
      "638/638 [==============================] - 28s 44ms/step - loss: 0.1324 - acc: 0.7508 - val_loss: 0.1159 - val_acc: 0.7750\n",
      "Epoch 15/25\n",
      "576/638 [==========================>...] - ETA: 2s - loss: 0.1294 - acc: 0.7587"
     ]
    }
   ],
   "source": [
    "for lr in learning_rates:\n",
    "\tfor bs in batch_sizes:\n",
    "\t\tfor optz in optz_choices:\n",
    "\t\t\tlayer1 = 100\n",
    "\t\t\tlayer2 = 50\n",
    "\n",
    "\t\t\tlog = '\\n'\n",
    "\t\t\tlog += (('\\n\\n==========\\nExecuting for %s layers and %s, with %s epochs, with %s learning rate, with %s batch size, with %s optimizer. Executed at %s')%(layer1,layer2,EPOCHS,lr,bs,optz,e2))\n",
    "\n",
    "\t\t\t# Create model\n",
    "\t\t\tmodel = Sequential()\n",
    "\t\t\tmodel.add(Lambda(lambda x: x/127.5-1.0, input_shape=SHAPE))\n",
    "\n",
    "\t\t\tmodel.add(Conv2D(36, (5, 5), activation='elu', strides=(2, 2)))\n",
    "\t\t\tmodel.add(Conv2D(24, (5, 5), activation='elu', strides=(2, 2)))\n",
    "\t\t\tmodel.add(Conv2D(48, (5, 5), activation='elu', strides=(2, 2)))\n",
    "\t\t\tmodel.add(Conv2D(64, (3, 3), activation='elu'))\n",
    "\t\t\tmodel.add(Conv2D(64, (3, 3), activation='elu'))\n",
    "\n",
    "\t\t\tmodel.add(Dropout(0.5))\n",
    "\t\t\tmodel.add(Flatten())\n",
    "\n",
    "\t\t\tmodel.add(Dense(layer1, input_dim=SHAPE, activation='elu'))\n",
    "\t\t\tmodel.add(Dense(layer2, activation='elu'))\n",
    "\t\t\tmodel.add(Dense(3))\n",
    "\n",
    "\t\t\tprint(model.summary())\n",
    "\t\t\tprint_step(\"Model defined!\",e2)\n",
    "\n",
    "\t\t\tprint(\">> Compiling model...\")\n",
    "\t\t\te3 = time.time()\n",
    "\n",
    "\t\t\t# Compile model\n",
    "\t\t\tlearn_rate = lr\n",
    "\t\t\tBATCH_SIZE = bs\n",
    "\t\t\tif (optz == 'SGD'):\n",
    "\t\t\t\topt = optimizers.SGD(lr=learn_rate)\n",
    "\t\t\telse:\n",
    "\t\t\t\topt = optimizers.Adam(lr=learn_rate)\n",
    "\n",
    "\t\t\tmodel.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "\t\t\tcheckpoint = ModelCheckpoint('model-{epoch:03d}.h5',\n",
    "\t                             monitor='val_loss',\n",
    "\t                             verbose=0,\n",
    "\t                             save_best_only=True,\n",
    "\t                             mode='auto')\n",
    "\n",
    "\t\t\tprint_step(\"Model compiled!\", e3)\n",
    "\n",
    "\t\t\tprint(\">> Fiting model...\")\n",
    "\t\t\te4 = time.time()\n",
    "\t\t\t# Fit the model\n",
    "\t\t\tmodel.fit(train\n",
    "\t\t\t\t\t, train_labels\n",
    "\t\t\t\t\t, validation_data=(test, test_labels)\n",
    "\t\t\t\t\t, epochs=EPOCHS\n",
    "\t\t\t\t\t, batch_size=BATCH_SIZE\n",
    "\t\t\t\t\t, verbose=1)\n",
    "\t\t\tlog += (('\\n \tTrain time: %s')%(time.time() - e4))\n",
    "\t\t\tprint_step(\"Fit finished!\", e4)\n",
    "\n",
    "\t\t\tprint(\">> Evaluating model...\")\n",
    "\t\t\te5 = time.time()\n",
    "\n",
    "\t\t\t# Evaluate train data\n",
    "\t\t\tscores_train = model.evaluate(train, train_labels)\n",
    "\t\t\teval_train = ((\"\\n \tEvaluate Train: %s: %.2f%%\") % (model.metrics_names[1], scores_train[1]*100))\n",
    "\t\t\tlog += '\\n'+eval_train\n",
    "\t\t\tprint(eval_train)\n",
    "\n",
    "\t\t\t# Evaluate test data\n",
    "\t\t\tscores_test = model.evaluate(test, test_labels)\n",
    "\t\t\teval_test = ((\"\\n \tEvaluate Test: %s: %.2f%%\") % (model.metrics_names[1], scores_test[1]*100))\n",
    "\t\t\tlog += '\\n'+eval_test\n",
    "\t\t\tprint(eval_test)\n",
    "\n",
    "\t\t\tlog += (('\\n\\n \tTotal time: %s')%(time.time() - e2))\n",
    "\n",
    "\t\t\tprint_step(\"Evaluation finished!\", e5)\n",
    "\n",
    "\t\t\tprint(\">> Saving model...\")\n",
    "\t\t\te6 = time.time()\n",
    "\n",
    "\t\t\t# save to json\n",
    "\t\t\tname = MODEL_PATH+'keras_model_'+str(lr)+'_learning_rate_'+str(bs)+'_batch_size_'+(optz)+'_optimizer_'+str(round(scores_test[1]*100))+'_acc'\n",
    "\t\t\tlog += (('\\n\\n \tsaved with name \"%s\"')%(name))\n",
    "\t\t\tmodel.save(name+\".model\")\n",
    "\n",
    "\t\t\twith open(MODEL_PATH+'models_log.txt','a') as log_file:\n",
    "\t\t\t\tlog_file.write(log)\n",
    "\n",
    "\t\t\tprint_step(\"Model saved!\", e6)\n",
    "\n",
    "print_step(\"Finished!!\", e0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
